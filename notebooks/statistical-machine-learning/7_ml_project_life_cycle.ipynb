{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c47887-4546-4852-b095-0cbe02569eb6",
   "metadata": {},
   "source": [
    "### <div align=\"center\">ML Project Life Cycle</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb99c17-f2a7-4956-a2be-690826ff042a",
   "metadata": {},
   "source": [
    "##### 10 Stages of AI Project Life Cycle\n",
    "1.Requirements -> 2.Data Collection -> 3.Data Preparation -> 4.Exploratory Data Analysis (EDA) -> 5.Feature Engineering -> 6.Model Selection & Training -> 7.Model Evaluation -> 8.Model Fine Tuning -> 9.Model Deployment -> 10.Monitoring & Feedback loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d7d39-d539-42ac-9503-13f7fa79e4a0",
   "metadata": {},
   "source": [
    "##### 1. Requirements and Scope of Work (SOW)\n",
    "- An SOW document is a mix of business requirement and the technology being used. This acts as a reference point for all stakeholders involved.\n",
    "- Project estimation needs to be done with a consideration that in the initial phase the requirements are vague, and the timeline estimate typically gets better as time passes.\n",
    "- JIRA is a popular software used in the industry to manage AI/ML projects.\n",
    "- Other popular tools used for project management are confluence, Notion, Asana, Excel etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa0dce-f7d2-4b02-b023-bccb562be752",
   "metadata": {},
   "source": [
    "##### 2. Data Collection\n",
    "- Data collection for an AI/ML project, typically happens from the data collected inside the organization.\n",
    "- In some cases, companies do buy additional data from vendors outside the organization.\n",
    "- Data can be extracted from website using python library like Beautiful Soup or a ready made software like BrightData."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943f09e4-9bf2-4db5-a034-fa775b73909a",
   "metadata": {},
   "source": [
    "##### 3 & 4. Data Cleaning & Exploratory Data Analysis (EDA)\n",
    "- In the data preparation stage following processes are typically done â€“ cleaning and transforming raw data into a format that can be analyzed, involving tasks like handling missing values, encoding categorical variables, and normalizing or scaling features.\n",
    "- EDA is performed to analyze the quality and characteristics of data with respect to the requirement of the model.\n",
    "- In Jupyter notebook you can run a python code to perform quick analysis and create visualizations.\n",
    "- Apache Spark takes care of work distribution of computing complex calculations to different computing nodes on cloud and aggregates the final answer. - As an end user, one would feel that every calculation is performed on the local computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd136f-7e72-4eab-b39d-6b9f0f52f3bc",
   "metadata": {},
   "source": [
    "##### 5. Feature Engineering\n",
    "- Feature engineering is a way to help the machine learning model with additional features derived from the given data so that it can make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b48821-5992-4863-be0d-75703914ab73",
   "metadata": {},
   "source": [
    "##### 6 & 7. Model Selection & Training and Model Evaluation\n",
    "- Model selection and training will be tailored to the specific use case and scenarios. Refer to the Model Selection Guide available in the download section to choose and train the appropriate model.\n",
    "- Model Selection & Training Tools: Python, scikit learn, PyTorch, Jupyter Notebook, XGBoost, TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d915669d-0910-4761-9535-a078fd1e6261",
   "metadata": {},
   "source": [
    "##### 8. Model Fine Tuning\n",
    "- Model evaluation and Model Fine Tuning will be performed together.\n",
    "- Model Evaluation Tools: Python, scikit learn, PyTorch, Jupyter Notebook, XGBoost, TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce756e8-3df7-40c9-ac81-8b9973aa391b",
   "metadata": {},
   "source": [
    "##### 9. Model Deployment\n",
    "- The trained model needs to be deployed on a server so that the model can be used by applications.\n",
    "- FastAPI, Flask, NodeJS are some servers in which the model can be deployed.\n",
    "- The end user application connects with the server to request or send data using API endpoints.\n",
    "- Any server or application using API service will provide API end points which the user can use to send or receive data.\n",
    "- AWS, Azure, Google Cloud are some of the leading providers to deploy the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1cb3c2-20c1-4adc-8181-0449bab3810a",
   "metadata": {},
   "source": [
    "##### 10. Monitoring and feedback using ML Ops \n",
    "- AI engineers write their code initially in the development environment.\n",
    "- The model moves to a staging or testing area where necessary checks are performed before moving it to production environment.\n",
    "- Once the model is deployed in production, monitoring is done to take feedback and improve the model.\n",
    "- The feedback is provided by combining feedback from both humans and algorithms.\n",
    "- A model is regularly updated with new datasets to avoid data drifts.\n",
    "- Model Deployment via Synapse Analytics (Check in details)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
